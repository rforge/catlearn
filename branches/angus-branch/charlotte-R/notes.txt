* Original email

Hi guys,

So I've done a version of COVIS in R (mainly couldn't be bothered to debug
in MATLAB and also have no ideas whether I'd miss a slight difference in
implementation between various COVIS versions). It's attached along with a
lovely graph. Sorry it's not optimised.

Several things I noticed when doing this that I didn't quiz Angus about
(you may have already dealt with this but thought would type out as well,
just in case):
1. The implementation of the category structure in terms of 0s and 1s
matters - especially for the II category structure. If you have 7/8th of
the stimuli the same as an optimum rule (i.e. going to the correct category
label) COVIS will learn the II structure. If, however, you do it the other
way around it will take a few thousand trials - not really what they were
hoping for I think.
2. I've added a whole bunch of []+ functions where they are not included in
the model (around the learning of the weights and saliences in the
rule-based) - otherwise I got negative saliences. However, this may just be
because I included a bug (now fixed), so will double check whether that's
necessary or not.
3. I've also included the learning criteria as used in WA2001: training
finishes if there are 8 correct trials in a row OR they hit 200 trials,
whichever comes first. I didn't ask Angus whether he had done the same.

Hope this was helpful,
I'm in University tomorrow if you both were free and would like to chat
about it all?

Charlotte

p.s. if you find a bug I haven't, could you let me know? Practice makes
perfect and all that

* Follow-up email

I have coherence.

Woot woot.

This is with 1000 trials for each bar :) (oh wait - it might be 500...still)

Still not angus and still not like Ashby.  That being said, it looks like
Ashby for everything but the rule-based system concurrent load - I guess my
positive hack for saliences was not what they used... :S

Anyhoo - the data is up on catlearn (2000 rows = one experiment)

Look forward to finding all my mistakes soon :)

* Andy's reply

Angus, Charlotte.

Thanks, Charlotte. So, that looks pretty stable. I've uploaded the 3
graphs and your email to the SVN.

Focussing on the smaller, more tractable issues first, whatever the
inclarities of the book chapter, the stick-out problem here is that
you and Angus are getting different results to each other. From
recollection, the main difference is that Angus has a major difference
between the two II conditions in his simulation.

I talked to Angus yesterday afternoon, and his plan is get his code to
produce a full model state output for every trial, which he can then
compare to your corresponding output.

This is a good plan, but one has to get to the point where one would
*expect* the two implementations to give the same answer at an
individual trial level. This needs two things, one easy, one hard:

Easy: The two implementations need to be given exactly the same
training items in exactly the same order.

Hard: They need to produce the same 'random' numbers when making a
call to a random number generator. It's possible that if Angus is
using R's RNGs that this will happen if you just specify a random
seed. Check it out. If not, there is a solution but it's more of a
pain - ask me. 

All the best

Andy
